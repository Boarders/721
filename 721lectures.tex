
% The following code introduces a new \if macro which we use to switch
% compilation between the published and internet versions of the book
%
\newif\ifmine
%
% The default is false, which means we compile the internet version.
% Un-comment the next line to compile the published version:
%
%\minetrue
%
\documentclass{amsart}
\usepackage[hidelinks]{hyperref}  
\usepackage{tensor}    
\usepackage{comment} 
\usepackage{enumitem}      
\usepackage{moreenum}
\usepackage{graphicx}   
\usepackage{ifthen}  
\usepackage{stmaryrd}
\usepackage[svgnames]{xcolor}  
 \usepackage{fullpage}
 \hypersetup{  
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
} 
\usepackage{mathpartir}%I think for \inferrule
 
% Font packages  
\usepackage[no-math]{fontspec} 
\usepackage{realscripts}

% Unicode mathematics fonts
\usepackage{unicode-math}
\setmathfont{Asana Math}[Alternate = 2]

% Font imports, for some reason this has to be after 
% the unicode-math stuff. 

\setmainfont{CormorantGaramond}[
Numbers = Lining,  
Ligatures = NoCommon,
Kerning = On,
UprightFont = *-Medium,
ItalicFont = *-MediumItalic,
BoldFont = *-Bold,
BoldItalicFont = *-BoldItalic
]

\setsansfont{texgyreheros}[
Scale=0.9129,
Ligatures = NoCommon,
UprightFont = *-Regular, 
ItalicFont = *-Italic,
BoldFont = *-Bold,
BoldItalicFont = *-BoldItalic
]

\setmonofont{SourceCodePro}[
Scale=0.8333,
UprightFont = *-Regular,
ItalicFont = *-MediumItalic,
BoldFont = *-Bold,
BoldItalicFont = *-BoldItalic
]

% AMS Packages
\usepackage{amsmath}
\usepackage{amsxtra}
\usepackage{amsthm}

% We use TikZ for diagrams
\usepackage{tikz}
\usepackage{tikz-cd}
\usepackage{makebox}%to try and fix the spacing in some diagrams with wildly divergent node sizes

\renewcommand{\theenumi}{\roman{enumi}} %roman numerals in enumerate

% Adjust list environments.

\setlist{}
\setenumerate{leftmargin=*,labelindent=0\parindent}
\setitemize{leftmargin=\parindent}%,labelindent=0.5\parindent}
%\setdescription{leftmargin=1em}

\newcommand{\todo}[1]
{ {\bfseries \color{blue} #1 }}


\newcommand{\lecture}[1]{\vspace{.1cm}\centerline{\fbox{\textbf{#1}}}\vspace{.1cm}}

\theoremstyle{theorem}
\newtheorem*{thm}{Theorem}
\newtheorem*{lem}{Lemma}
\newtheorem*{fact}{Fact}
\newtheorem*{cor}{Corollary}
\newtheorem*{prop}{Proposition}

\theoremstyle{definition}
\newtheorem*{defn}{defn}
\newtheorem*{ntn}{Notation}
\newtheorem*{post}{Postulate}
\newtheorem*{ex}{ex}
\newtheorem*{nex}{non-ex}
\newtheorem*{exc}{Exercise}
\newtheorem*{exnex}{Example/Non-Example}
\newtheorem*{tf}{T/F}
\newtheorem*{q}{Q}
\newtheorem*{rQ}{rhetorialQ}
\newtheorem*{rev}{Review}


\theoremstyle{remark}
\newtheorem*{rmk}{Remark}
\newtheorem*{war}{Warning}
\newtheorem*{dig}{Digression}

%\makeatletter
%\let\c@equation\c@thm
%\makeatother
%\numberwithin{equation}{section}

\newcommand{\cat}[1]{\textup{\textsf{#1}}}% for categories
\newcommand{\fun}[1]{\textup{#1}}%for functors

%commonly used categories
\newcommand{\Ch}{\cat{Ch}}
\newcommand{\Set}{\cat{Set}}
\newcommand{\sSet}{\cat{sSet}}
\newcommand{\Top}{\cat{Top}}

%math operators
\DeclareMathOperator{\dom}{\mathrm{dom}}
\DeclareMathOperator{\cod}{\mathrm{cod}}
\DeclareMathOperator{\ob}{\mathrm{ob}}
\DeclareMathOperator{\mor}{\mathrm{mor}}
\DeclareMathOperator*{\colim}{\mathrm{colim}}
\newcommand{\hocolim}{\mathrm{hocolim}}
\newcommand{\wcolim}{\mathrm{wcolim}}
\newcommand{\holim}{\mathrm{holim}}


\newcommand{\op}{\mathrm{op}}
\newcommand{\co}{\mathrm{co}}
\newcommand{\Nat}{\mathrm{Nat}}
\newcommand{\End}{\mathrm{End}}
\newcommand{\Aut}{\mathrm{Aut}}
\newcommand{\Sym}{\mathrm{Sym}}

\newcommand{\ev}{\mathrm{ev}}
\newcommand{\im}{\mathrm{im}}
\newcommand{\coim}{\mathrm{coim}}
\newcommand{\To}{\Rightarrow}
\newcommand{\coker}{\mathrm{coker}}

\newcommand{\Map}{\mathord{\text{\normalfont{\textsf{Map}}}}}
\newcommand{\Fun}{\mathord{\text{\normalfont{\textsf{Fun}}}}}
\newcommand{\Hom}{\mathord{\text{\normalfont{\textsf{Hom}}}}}
\newcommand{\Ho}{\mathord{\text{\normalfont{\textsf{Ho}}}}}
\newcommand{\h}{\cat{h}}
\DeclareMathOperator{\Lan}{\fun{Lan}}
\DeclareMathOperator{\Ran}{\fun{Ran}}
\newcommand{\comma}{\!\downarrow\!}

%special blackboard bold characters
\newcommand{\bbefamily}{\fontencoding{U}\fontfamily{bbold}\selectfont}
\newcommand{\textbbe}[1]{{\bbefamily #1}}
\DeclareMathAlphabet{\mathbbe}{U}{bbold}{m}{n}


\def\DDelta{{\mbfDelta}}

%categories?
\newcommand{\cA}{\mathsf{A}}
\newcommand{\cB}{\mathsf{B}}
\newcommand{\cC}{\mathsf{C}}
\newcommand{\cD}{\mathsf{D}}
\newcommand{\cE}{\mathsf{E}}
\newcommand{\cF}{\mathsf{F}}
\newcommand{\cG}{\mathsf{G}}
\newcommand{\cI}{\mathsf{I}}
\newcommand{\cJ}{\mathsf{J}}
\newcommand{\cK}{\mathsf{K}}
\newcommand{\cL}{\mathsf{L}}
\newcommand{\cM}{\mathsf{M}}
\newcommand{\cN}{\mathsf{N}}
\newcommand{\cP}{\mathsf{P}}
\newcommand{\cS}{\mathsf{S}}
\newcommand{\cT}{\mathsf{T}}
\newcommand{\cV}{\mathsf{V}}

\newcommand{\0}{\mathbbe{0}}
\newcommand{\1}{\mathbbe{1}}
\newcommand{\2}{\mathbbe{2}}
\newcommand{\3}{\mathbbe{3}}
\newcommand{\4}{\mathbbe{4}}
\newcommand{\iso}{\mathbbe{I}}

%blackboard bold
\renewcommand{\AA}{\mathbb{A}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\LL}{\mathbb{L}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\kk}{\mathbbe{k}}

%
\newcommand{\we}{\mathcal{W}}
\newcommand{\cof}{\mathcal{C}}
\newcommand{\fib}{\mathcal{F}}
\newcommand{\mono}{\mathcal{M}}
\newcommand{\epi}{\mathcal{E}}
\newcommand{\sk}{\mathrm{sk}}

\newcommand{\sA}{\mathcal{A}}
\newcommand{\sB}{\mathcal{B}}
\newcommand{\sC}{\mathcal{C}}
\newcommand{\sJ}{\mathcal{J}}
\newcommand{\sL}{\mathcal{L}}
\newcommand{\sR}{\mathcal{R}}



%type theory stuff
\newcommand{\univ}{{~\texttt{type}~}}
\newcommand{\judgment}{\mathcal{J}}
\newcommand{\term}[1]{{\textup{\texttt{#1}}}}
\newcommand{\type}[1]{{\textup{#1}}}

\newcommand{\comp}{\term{comp}}
\newcommand{\id}{\term{id}}

\newcommand{\bN}{{\mathbb{N}}}
\newcommand{\suc}{\term{succ}_{\bN}}

\newcommand{\ind}{\term{ind}}
\newcommand{\inl}{\term{inl}}
\newcommand{\inr}{\term{inr}}
\newcommand{\pair}{\term{pair}}
\newcommand{\pr}{\term{pr}}

\newcommand{\bZ}{{\mathbb{Z}}}

\newcommand{\refl}{\term{refl}}
\newcommand{\pathind}{\term{path}\text{-}\term{ind}}
\newcommand{\concat}{\term{concat}}
\newcommand{\inv}{\term{inv}}
\newcommand{\assoc}{\term{assoc}}
\newcommand{\ap}{\term{ap}}
\newcommand{\apcoh}[1]{\term{ap}\text{-}\term{#1}}
\newcommand{\tr}{\term{tr}}
\newcommand{\apd}{\term{apd}}

\newcommand{\UU}{\mathcal{U}}
\newcommand{\sT}{\mathcal{T}}
\newcommand{\Id}{\textup{\text{Id}}}
\newcommand{\Eq}{\textup{\text{Eq}}}



\begin{document}

\title{Math 721: Homotopy Type Theory}
\author{Emily Riehl}
\date{Fall 2021}

%\begin{abstract}
%\end{abstract}

\address{Dept.~of Mathematics\\Johns Hopkins University\\3400 N Charles St\\Baltimore, MD 21218}
\email{eriehl@math.jhu.edu}

\maketitle

\setcounter{tocdepth}{1}
\tableofcontents


\ifmine
\subsection*{Index cards:} name, what I should call you, pronouns, year, relevant previous coursework, why you are here, something fun


\subsection*{Personal history:}
Prehistory of homotopy type theory dates to a 1998 paper of Martin Hofmann and Thomas Streicher called ``The groupoid interpretation of type theory,'' that I'll tell you about later. Key early developments were in the mid aughts with the major players coming together for the first time in 2010 at Carnegie Mellon. This expanded to a larger group at Oberwolfach in 2011 followed by a special year at IAS in 2012-2013, during which the HoTT book (not our book) was written.

I got my PhD in 2011. I started hearing a little about this in my final years of grad school but learned most of what I'll tell you about this semester during my postdoc and while here at Johns Hopkins. Point of this to say is that learning doesn't stop when you earn your PhD. Learning also doesn't necessarily precede teaching. My goal for this semester is to know a lot more \texttt{agda} by the end than I do right now. My promise to you is that I'll do all the homework before I assign it. I don't promise that I'll be able to answer very many of your questions about what is going on under the hood, but that's okay too: it's probably useful for all of us to be reminded that figures of authority don't always know what they're talking about.

\subsection*{Syllabus:}
The goal for this course is to change the way you think about doing mathematics. I certainly have. Along the way, I hope you all learn a lot, though as is typical in a graduate course, that's somewhat in proportion to the amount of work you put in. This also has a lot to do with your background. If you're coming from CS, you'll probably leave the course knowing way more \texttt{agda} than I do, but perhaps a bit less about synthetic homotopy theory. For others, it will be vice versa. Still others might engage most with the philosophical implications of these developments. I'm equally happy with all of these directions. 

There are two ``active learning'' activities. The first is writing your own formal proofs in the computer proof assistant \texttt{agda}. This is hard in the sense that if you make one typo the whole thing breaks but a lot of help is available. I'll say more about it when the time comes. In particular, I'm hoping many folks will attend group office hours on Thursday evenings from 5-6pm. In the first meeting there won't be a problem set due: instead the goal will be to get \texttt{agda} installed. Come see me if you're worried about technical issues.

The second active learning activity is a final project. I'm very flexible about the parameters. Essentially you should pick something that sounds fun to you.

Questions?
\fi

\part{Martin-L\"of's Dependent Type Theory}


\section*{August 30: Dependent Type Theory}

Martin-L\"{o}f's dependent type theory is a formal language for writing mathematics: both constructions of mathematical objects and proofs of mathematical propositions. As we shall discover, these two things are treated in parallel (in contrast to classical Set theory plus first-order logic, where the latter supplies the proof calculus and the former gives the language which you use to state things to prove).

\subsection*{Judgments and contexts}

I find it helpful to imagine I'm teaching a computer to do mathematics. It's also helpful to forget that you know other ways of doing mathematics.\footnote{Indeed, there are very deep theorems that describe how to interpret dependent type theory into classical set-based mathematics. You're welcome to investigate these for your final project but they are beyond the scope of this course.}

\begin{defn} There are four kinds of \textbf{judgments} in dependent type theory, which you can think of as the ``grammatically correct'' expressions:
\begin{enumerate}
\item $\Gamma \vdash A \univ$, meaning that $A$ is a well-formed type in \textbf{context} $\Gamma$ (more about this soon).
\item $\Gamma \vdash a : A$, meaning that $a$ is a well-formed term of type $A$ in context $\Gamma$.
\item $\Gamma \vdash A \doteq B \univ$, meaning that $A$ and $B$ are \textbf{judgmentally} or \textbf{definitionally} equal types in context $\Gamma$.
\item $\Gamma \vdash a \doteq b : A$, meaning that $a$ and $b$ are judgmentally equal terms of type $A$ in context $\Gamma$.
\end{enumerate}
These might be collectively abbreviated by $\Gamma \vdash \judgment$.
\end{defn}

The statement of a mathematical theorem, often begins with an expression like ``Let $n$ and $m$ be positive integers, with $n < m$, and let $\vec{v}_1,\ldots, \vec{v}_m$ be vectors in $\RR^n$. Then \ldots'' This statement of the hypotheses defines a \textbf{context}, a finite list of types and hypothetical terms (called \textbf{variables}\footnote{We're not going to say anything about proper syntax for variables and instead rely on instinct to recognize proper and improper usage.}) satisfying an inductive condition that that each type can be derived in the context of the previous types and terms using the inference rules of type theory.

\begin{defn} A \textbf{context} is a finite list of variable declarations:
\[ x : A_1, x_2 : A_2(x_1), \ldots, x_n : A_n(x_1,\ldots, x_{n-1})\]
satisfying the condition that for each $1\leq k \leq n$ we can derive the judgment
\[ x_1 : A_1, \ldots, x_{k-1} : A_{k-1}(x_1,\ldots, x_{k-2}) \vdash A_k(x_1,\ldots, x_{k-1}) \univ\]
using the inference rules of type theory.
\end{defn}

We'll introduce the inference rules shortly but the idea is that it needs to be possible to form the type $A_k(x_1,\ldots, x_{k-1})$ given terms $x_1, \ldots, x_{k-1}$ of the previously-formed types. 

\begin{ex} For example, there is a unique context of length zero: the empty context.
\end{ex}

\begin{ex} $n : \NN, m : \NN, p : n < m, \vec{v} : (\RR^n)^m$ is a context. Here $n : \NN, m : \NN \vdash n < m$ is a dependent type that corresponds to the relation $\{ n < m \mid n, m \in \NN\} \subset \NN \times \NN$ and the variable $p$ is a witness that $n < m$ is true (more about this later). 
\end{ex}

\subsection*{Type families}

Absolutely everything in dependent type theory is context dependent so we always assume we're working in a background context $\Gamma$. Let's focus on the primary two judgment forms.

\begin{defn} Given a type $A$ in context $\Gamma$ a \textbf{family} of types over $A$ in context $\Gamma$ is a type $B(x)$ in context $\Gamma, x : A$, as represented by the judgment:
\[ \Gamma, x : A \vdash B(x) \univ\]
We also say that $B(x)$ is a type \textbf{indexed} by $x : A$, in context $\Gamma$.
\end{defn}

\begin{ex}  $\RR^n$ is a type indexed by $n \in \NN$.
\end{ex}

\begin{defn} Consider a type family $B$ over $A$ in context $\Gamma$. A \textbf{section} of the family $B$ over $A$ in context $\Gamma$ is a term of type $B(x)$ in context $\Gamma, x : A$, as represented by the judgment:
\[ \Gamma, x : A \vdash b(x) : B(x) \]
We say that $b$ is a \textbf{section} of the family $B$ over $A$ in context $\Gamma$ or that $b(x)$ is a term of type $B(x)$ indexed by $x : A$ in context $\Gamma$.
\end{defn}

\begin{ex} $\vec{0}_n : \RR^n$ is a term dependent on $n \in \NN$.
\end{ex}

\begin{exc} If you've heard the word ``section'' before you should think about what it is being used here.
\end{exc}

\subsection*{Inference rules}

There are five types of inference rules that collectively describe the structural rules of dependent type theory. They are
\begin{enumerate}
\item Rules postulating that judgmental equality is an equivalence relation:
\[
\inferrule{\Gamma \vdash A \univ}{\Gamma \vdash A \doteq A \univ}\quad
\inferrule{\Gamma \vdash A \doteq B \univ}{\Gamma \vdash B \doteq A \univ}\quad
\inferrule{\Gamma \vdash A \doteq B \univ \\ \Gamma \vdash B \doteq C \univ}{\Gamma \vdash A \doteq C \univ}
\]
and similarly for judgmental equality between terms.
\item Variable conversion rules for judgmental equality between types:
\[
\inferrule{\Gamma \vdash A \doteq A' \univ \\ \Gamma, x : A, \Delta \vdash  \judgment}{\Gamma, x : A', \Delta \vdash \judgment}
\]

\item Substitution rules:
\[
\inferrule{\Gamma \vdash a : A \\ \Gamma, x : A, \Delta \vdash \judgment}{\Gamma, \Delta[a/x] \vdash \judgment[a/x]}
\]
If $\Delta$ is the context $y_1 : B_1(x),\ldots, y_n : B_n(x,y_1,\ldots, y_{n-1})$ then $\Delta[a/x]$ is the context $y_1 : B(a), \ldots, y_n : B_n(a,y_1,\ldots, y_{n-1})$. A similar substitution is performed in the judgment $\judgment[a/x]$. Further rules indicate that substitution by judgmentally equal terms gives judgmentally equal results.
\item Weakening rules:
\[
\inferrule{ \Gamma \vdash A \univ \\ \Gamma,\Delta \vdash \judgment}{\Gamma, x : A, \Delta \vdash \judgment}
\]
Eg if $A$ and $B$ are types in context $\Gamma$, then $B$ is also a type in context $\Gamma, x : A$.
\item The generic term:
\[ 
\inferrule{\Gamma \vdash A \univ }{\Gamma, x : A \vdash x :A}
\]
This will be used to define the identity function of any type.
\end{enumerate}

\subsection*{Derivations}

A derivation in type theory is a finite rooted tree where each node is a valid rule of inference. The root is the conclusion.

\begin{ex} The interchange rule is derived as follows
\[
\inferrule{ 
\inferrule{ 
\inferrule{ \Gamma \vdash B \univ}{\Gamma, y :B \vdash y : B}}
{\Gamma, y : B, x : A\vdash y : B} \qquad
\inferrule{
\inferrule{}{\Gamma \vdash B \univ} \\ \inferrule{\Gamma, x : A, y : B, \Delta \vdash \judgment}{\Gamma, x : A, z : B, \Delta[z/y] \vdash \judgment[z/y]}}
{\Gamma, y : B, x : A, z : B, \Delta[z/y] \vdash \judgment[z/y]}
}{\Gamma, y : B, x : A, \Delta \vdash \judgment}
\]
\end{ex}

\section*{September 1: Dependent function types \& the natural numbers}

\subsection*{The rules for dependent function types}

Consider a section $b$ of a family $B$ over $A$ in context $\Gamma$, as encoded by a judgment:
\[ \Gamma, x : A \vdash b(x) : B(x).\]
We think of the section $b$ as a function that takes as input $x : A$ and produces a term $b(x) : B(x)$. Since the type of the output is allowed to depend on the term being input, this isn't quite an ordinary function but a \textbf{dependent function}. The type of all dependent functions is the \textbf{dependent function type}
\[ \prod_{x : A} B(x)\]

What is a thing in mathematics? Structuralism says the ontology of a thing is determined by its behavior. In dependent type theory, we define dependent function types by stating their rules, which have the following forms:
\begin{enumerate}
\item \textbf{formation rules} tell us how a type may be formed
\item \textbf{introduction rules}  tell us how to introduce new terms of the type
\item \textbf{elimination rules}  tell us how the terms of a type may be used
\item  \textbf{computation rules} tell us how the introduction and elimination rules interact
\end{enumerate}
There are also \textbf{congruence rules} that tell us that all constructions respect judgmental equality. See your book for more details.

\begin{defn}[dependent function types]
The $\Pi$-\textbf{formation rule} has the form:
\[
\inferrule{ \Gamma, x : A \vdash B(x) \univ}{\Gamma \vdash \prod_{x :A} B(x) \univ}
\]

The $\Pi$-\textbf{introduction rule} has the form:
\[
\inferrule{ \Gamma, x : A \vdash b(x) : B(x)}{ \Gamma \vdash \lambda x. b(x) : \prod_{x :A} B(x)}
\]
The $\lambda$-\textbf{abstraction} $\lambda x. b(x)$ can be thought of as notation for $x \mapsto b(x)$. 

The $\Pi$-\textbf{elimination rule} has the form of the evaluation function:
\[
\inferrule{ \Gamma \vdash f : \prod_{x :A} B(x)}{\Gamma, x :A \vdash f(x) : B(x)}
\]

Finally, there are two computation rules: the $\beta$-\textbf{rule}
\[
\inferrule{\Gamma , x :A \vdash b(x) : B(x) }{ \Gamma, x : A \vdash (\lambda y. b(y))(x) \doteq b(x) : B(x)}
\]
and the $\eta$-\textbf{rule}, which says that all elements of a $\Pi$-type are dependent functions:
\[
\inferrule{ \Gamma \vdash f : \prod_{x : A}B(x)}{\Gamma \vdash \lambda x. f(x) \doteq f : \prod_{x :A} B(x)}\]
\end{defn}





\subsection*{Ordinary function types}

\begin{defn}[function types]
The formation rule is derived from the formation rule for $\Pi$-types together with weakening:
\[
\inferrule{
\inferrule{ \Gamma \vdash A \univ \\ \Gamma \vdash B \univ}{\Gamma, x : A \vdash B \univ}}
{\Gamma \vdash \prod_{x : A}B \univ}
\]
We adopt the notation
\[ A \to B \coloneq \prod_{x : A} B\]
for the dependent function type in the case where the type family $B$ is constant over $x : A$.

The introduction, evaluation, and computation rules are instances of term conversion: eg
\[
\inferrule{ \Gamma \vdash B \univ \\ \Gamma, x : A \vdash b(x) : B}
{ \Gamma \vdash \lambda x. b(x) : A \to B}
\qquad
\inferrule{ \Gamma \vdash f : A \to B}{\Gamma, x : A \vdash f(x) : B}
\]
plus the two computation rules:
\[
\inferrule{ \Gamma \vdash B \univ \\ \Gamma, x : A \vdash b(x) : B}{ \Gamma, x : A \vdash (\lambda y. b(y))(x) \doteq b(x) : B}
\qquad
\inferrule{ \Gamma \vdash f : A  \to B}{\Gamma \vdash \lambda x. f(x) \doteq f : A \to B}
\]
\end{defn}

\begin{defn} Identity functions are defined as follows:
\[
\inferrule{
\inferrule{ \Gamma \vdash A \univ}{\Gamma, x : A \vdash x : A}}
{ \Gamma \vdash \lambda x. x : A \to A}\]
which is traditionally denoted by $\id_A \coloneq \lambda x. x$.
\end{defn}

The idea of composition is that given a function $f \colon A \to B$ and $g \colon B \to C$ you should get a function $g \circ f \colon A \to C$. Using infix notation you might denote this function by $\_\circ\_$.

\begin{q} $\_\circ\_$ is itself a function, so it's a term of some type. What type?\footnote{Really the type should involve three universe variables but let's save this for next week.}
\end{q}

\begin{defn} Composition has the form:
\[
\inferrule{ \Gamma \vdash A \univ \\ \Gamma \vdash B \univ \\ \Gamma \vdash C \univ}{\Gamma \vdash \_\circ\_ : (B \to C) \to ((A \to B) \to (A \to C))}
\]
It is defined by
\[ \_\circ\_ \coloneq \lambda g. \lambda f . \lambda x. g(f(x))\]
which can be understood as the term constructed by three applications of the $\Pi$-introduction rule followed by two applications of the $\Pi$-elimination rule.
\end{defn}

Composition is associative essentially because both $(h\circ g)\circ f$ and $h \circ (g \circ f)$ are defined by $\lambda x. h(g(f(x)))$. We'll think about this more formally when we come back to identity types.

Similarly, you can compute that for all $f : A \to B$, $\id_B \circ f \doteq f : A \to B$ and $f \circ \id_A \doteq f : A \to B$.

\subsection*{The type of natural numbers}

The type $\bN$ of natural numbers is the archetypical example of an \textbf{inductive type} about more which soon. It is given by rules which say that it has a term $0_\bN : \bN$, it has a successor function $\suc : \bN \to\bN$ and it satisfies the induction principle.

The $\bN$-formation rule is
\[
\inferrule{~}{\vdash \bN \univ}
\]
In other words, $\bN$ is a type in the empty context.

There are two $\bN$-introduction rules:
\[
\inferrule{~}{\vdash 0_\bN : \bN} \qquad
\inferrule{~}{\vdash \suc : \bN \to \bN}
\]

\begin{dig}[traditional induction]
In traditional first-order logic, the principle of $\bN$-induction is stated in terms of a \textbf{predicate} $P$ over $\bN$. One way to think about $P$ is as a function $P \colon \bN \to \{ \top, \bot\}$. That is, for each $n \in \bN$, $P(n)$ is either true or false. We could also think of $P$ as an indexed family of sets $(P(n))_{n \in \bN}$ where for each $n$ either $P(n) = \emptyset$ (corresponding to $P(n)$ being false) or $P(n) = *$ (corresponding to $P(n)$ being true).

The induction principle then says \[ \forall P : \{0,1\}^\bN, (P(0) \wedge (\forall n, P(n) \to P(n+1)) \to \forall n, P(n)).\]
\end{dig}

In dependent type theory it is most natural to let $P$ be an arbitrary type family over $\bN$. This is a stronger assumption, as we'll see. 

\begin{q}
What then corresponds to a proof that $\forall n, P(n)$?
\end{q}

The induction principle is encoded by the following rule:
\[
\inferrule{ \Gamma, n : \bN \vdash P(n) \univ \\ \Gamma \vdash p_0 : P(0_\bN) \\ \Gamma \vdash p_S : \prod_{n : \bN} (P(n) \to P(\suc(n))) }
{ \Gamma \vdash \term{ind}_\bN(p_0,p_S) : \prod_{n : \bN} P(n)}
\]

\begin{rmk} There are other forms this rule might take that are interderivable with this one.
\end{rmk}

The computation rules say that the function $\term{ind}_\bN(p_0,p_S) : \prod_{n : \bN} P(n)$ behaves like it should on $0_\bN$ and successors:
\[
\inferrule{ \Gamma , n : \bN \vdash P(n) \univ \\ \Gamma \vdash p_0 : P(0_\bN) \\ \Gamma \vdash p_S : \prod_{n : \bN} (P(n) \to P(\suc(n))) }
{ \Gamma \vdash \term{ind}_\bN(p_0,p_S)(0_\bN) \doteq p_0 : P(0_\bN)}
\]
and under the same premises
\[ \Gamma, n : \bN \vdash \texttt{ind}_\bN(p_0, p_S)(\suc(n)) \doteq p_S(n, \texttt{ind}_\bN(p_0,p_S,n)) : P(\suc(n)).\]
These computation rules don't matter so much if the type family $n : \bN \vdash P(n)$ is really a predicate --- $P(n)$ is either true or false and that's the end of the story --- but they do matter if $P(n)$ is more like an indexed family of sets. In the latter case, $\term{ind}_\bN(p_0,p_S)$ is the recursive function defined from $p_0$ and $p_S$ and these are the computation rules for that recursion.

\begin{rmk} Recall Peano's axioms for the natural numbers: 
\begin{enumerate}
\item $0_\bN \in \bN$
\item $\suc : \bN \to \bN$
\item $\forall n, \suc(n) \neq 0_\bN$
\item $\forall n,m, \suc(n)= \suc(m) \to n=m$
\item induction
\end{enumerate}
We'll be able to \emph{prove} the missing two axioms from the induction principle we've assumed once we have identity types and universes. We'll come back to this in a few weeks.
\end{rmk}

\subsection*{Addition on the natural numbers}

\begin{rmk} When addition is defined by recursion on the second variable, from the computation rules associated to function types and the natural numbers type you can derive judgmental equalities \[
m + 0 \doteq m \quad \text{and} \quad m + \suc(n) \doteq \suc(m + n).
\]
But you can't derive the symmetric judgmental equalities.
\end{rmk}

We \emph{will} be able to prove such equalities using the identity types, to be introduced shortly.

\subsection*{Pattern matching}

To define a dependent function $f : \prod_{n : \bN} P(n)$ by induction on $n$ it suffices, by the elimination rule for the natural numbers type, to provide two terms:
\[ p_0 : P(0_\bN) \qquad p_S : \prod_{n : \bN} P(n) \to P(\suc(n)).\]
Thus the definition of $f$ may be presented by writing
\[ f(0_\bN) \coloneq p_0 \qquad f(\suc(n)) \coloneq p_S(n,f(n)).\]
This defines the function $f$ by \textbf{pattern matching} on the variable $n$. When a function is defined in this form, the judgmental equalities accompanying the definition are immediately displayed.

\section*{September 8: The formal proof assistant \texttt{agda}}

See \url{https://github.com/emilyriehl/721/blob/master/introduction.agda}


\section*{September 13: Inductive types}

The rules for the natural numbers type $\bN$ tell us:
\begin{enumerate}
\item how to form terms in $\bN$, and
\item how to define dependent functions in $\prod_{n : \bN} P(n)$ for any type family $n : \bN \vdash P(n) \univ$,
\end{enumerate}
while providing two computation rules for those dependent functions.

Many types can be specified by stating how to form their terms and how to define dependent functions out of them. Such types are called \textbf{inductive types}. 

\subsection*{The idea of inductive types}

Recall a type is specified by its formation rules, its introduction rules, its elimination rules, and its computation rules. For inductive types, the introduction rules specify the \textbf{constructors} of the inductive type, while the elimination rule provides the \textbf{induction principle}. The computation rules provide definitional equalities for the induction principle.

In more detail:
\begin{enumerate}
\item The constructors tell us what structure the identity type is given with.
\item The induction principle defines sections of any type family over the inductive type by specifying the behavior at the constructors.
\item The computation rules assert that the inductively defined section agrees on the constructors with the data used to define it. So there is one computation rule for each constructor.
\end{enumerate}

\subsection*{The unit type}

The formal definition of the \textbf{unit type} is as follows:
\[
\inferrule{}{\vdash \1 \univ} \qquad
\inferrule{}{\vdash \star : \1} \qquad 
\inferrule{ x : \1 \vdash P(x) \univ \\ p : P(\star)}{ x: \1 \vdash \ind_\1(p,x) : P(x)} \qquad
\inferrule{ x : \1 \vdash P(x) \univ \\ p : P(\star)}{x : \1 \vdash \ind_1(p,\star) \doteq p : P(\star)}
\]
As an inductive type, the definition is packaged as follows:

\begin{defn} The \textbf{unit type} is a type $\1$ equipped with a term $\star : \1$ satisfying the inductive principle that for any family $x : \1 \vdash P(x)$ there is a function
\[ \ind_\1 : P(\star) \to \prod_{x : 1} P(x)\]
with the computation rule $\ind_1(p,\star) \doteq p$.
\end{defn}

In agda, this definition has the form:
\begin{quote}
\texttt{data unit : UU lzero where\\ \indent
  star : unit}
  \end{quote}


\begin{q} What does the induction rule look like for a constant type family $A$ that does not depend on $\1$?
\end{q}

\subsection*{The empty type}

\begin{defn}
The empty type is a type $\emptyset$ satisfying the induction principle that for any family of types $x : \emptyset \vdash P(x)$ there is a term
\[ \ind_\emptyset : \prod_{x : \emptyset} P(x).\]
\end{defn}

That is the empty type is the inductive type with no constructors. Thus there are no computation rules. In agda, this definition has the form:
\begin{quote}
\texttt{data empty : UU lzero where}
  \end{quote}

\begin{rmk} As a special case of the elimination rule for the empty type we have
\[
\inferrule{ \vdash A \univ}{ \term{ex-falso} \coloneq \ind_\emptyset : \emptyset \to A}
\]
By the elimination rule for function types it follows that if we had a term $x : \emptyset$ then we could get a term in any type. The name comes from latin \emph{ex falso quodlibet}: ``from falsehood, anything.''
\end{rmk}

We've already seen a few glimpses of logic in type theory, something we'll discuss more formally soon. The basic idea is that we can interpret the formation of a type as akin to the process of formulating a mathematical statement that could be a sentence (if its a type in the empty context) or a predicate (if it's a dependent type). The act of constructing a term in that type is then analogous to proving the proposition so-encoded. 
These ideas motivate the logically-inflected terms in what follows. 

For instance, we can use the empty type to define a negation operation on types:
\begin{defn}
For any type $A$, we define its \textbf{negation} by $\neg A \coloneq A \to \emptyset$ and say the type $A$ \textbf{is empty} if there is a term in this type.
\end{defn}

\begin{rmk} To construct a term of type $\neg A$, use the introduction rule for function types and assume given a term $a : A$. The task then is to derive a term of $\emptyset$. In other words, we prove $\neg A$ by assuming $A$ and deriving a contradiction. This proof technique is called \textbf{proof of negation}.

This should be contrasted with \textbf{proof by contradiction}, which aims to prove a proposition $P$ by assuming $\neg P$ and deriving a contradiction. This uses the logical step ``$\neg \neg P$ implies $P$.'' In type theory, however, $\neg \neg A$ is the type of functions
\[ \neg \neg A \coloneq (A \to \emptyset) \to \emptyset)\] and it is not possible in general to use a term in this type to construct a term of type $A$. 
\end{rmk}

The law of contraposition does work, at least in one direction.

\begin{prop} For any types $P$ and $Q$ there is a function
\[ (P \to Q) \to (\neg Q \to \neg P).\]
\end{prop}
\begin{proof}
By $\lambda$-abstraction assume given $f : P \to Q$ and $\tilde{q} : Q \to \emptyset$. We seek a term in $P \to \emptyset$, which we obtain simply by composing: $\tilde{q} \circ f : P \to \emptyset$. Thus
\[\lambda f. \lambda \tilde{q} .\lambda p. \tilde{q}(f(p)) : (P \to Q) \to (\neg Q \to \neg P). \qedhere\]
\end{proof}

\subsection*{Coproducts}

Inductive types can be defined outside the empty context. For instance, the formation and introduction rules for the coproduct type have the form:
\[
\inferrule{ \Gamma \vdash A \univ \\ \Gamma \vdash B \univ}{\Gamma \vdash A + B \univ} 
\]
\[
\inferrule{ \Gamma \vdash A \univ \\ \Gamma \vdash B \univ \\ \Gamma \vdash a : A}{ \Gamma \vdash \inl a : A + B} \qquad
\inferrule{ \Gamma \vdash A \univ \\ \Gamma \vdash B \univ \\ \Gamma \vdash b : B}{ \Gamma \vdash \inr b : A + B}
\]

\begin{defn} Given types $A$ and $B$ the \textbf{coproduct type} is the type equipped with 
\[ \inl : A \to A + B \qquad \inr : B \to A+ B\]
satisfying the induction principle that says that for any family of types $x : A + B \vdash P(x) \univ$ there is a term
\[ \ind_+ : \left( \prod_{x : A} P(\inl (x))\right) \to \left( \prod_{y : B} P(\inr(y))\right) \to \prod_{z : A + B}P(z)\]
satisfying the computation rules
\[ \ind_+(f,g, \inl(x)) \doteq f(x) \qquad \ind_+(f,g, \inr(y)) \doteq g(y).\]
\end{defn}
Not as a special case we have
\[ \ind_+ : (A \to X) \to (B \to X) \to (A + B \to X)\]
which is similar to the elimination rule for disjunction in first order logic: if you've proven that $A$ implies $X$ and that $B$ implies $X$ then you can conclude that $A$ or $B$ implies $X$.

\subsection*{The type of integers}

There are many ways to define the integers in Martin-L\"{o}f type theory, one of which is as follows:

\begin{defn} Define the \textbf{integers} to be the type $\bZ \coloneq \bN + (\1 + \bN)$ which comes equipped with inclusions:
\[ \term{in-pos} \coloneq \inr\circ\inr : \bN \to \bZ \qquad \term{in-neg} \coloneq \inl : \bN \to \bZ\]
and constants
\[ -1_\bZ \coloneq \term{in-neg}(0_\bN) \qquad 0_\bZ \coloneq \inr (\inl(\star)) \qquad 1_{\bZ} \coloneq \term{in-pos}(0_\bN).\]
\end{defn}

Since $\bZ$ is built from inductive types it is then an inductive type given with its own induction principle.

\subsection*{Dependent pair types}

Of all the inductive types we've introduced, the final one is perhaps the most important.

Recall a \textbf{dependent function} $\lambda x . f(x) : \prod_{x : A}B(x)$ is like an ordinary function except the output type is allowed to vary with the input term. Similarly, a \textbf{dependent pair} $(a,b) : \sum_{x : A}B(x)$ is like an ordinary (ordered) pair except the type of the second term $b : B(a)$ is allowed to vary with the first term $a :A$.

\begin{defn} Consider a type family $x : A \vdash B(x) \univ$. The \textbf{dependent pair type} or $\Sigma$-\textbf{type} $\sum_{x :A}B(x)$ is the inductive type equipped with the function
\[ \pair : \prod_{x : A} \left(B(x) \to \prod_{y : A}B(y)\right).\]
The induction principle asserts that for any family of types $p : \sum_{x :A} B(x) \vdash P(p) \univ$ there is a function
\[ \ind_\Sigma : \left( \prod_{x :A} \prod_{y : B}P(\pair(x,y) \right) \to \left( \prod_{z : \sum_{x :A}B(x)}P(z) \right)
\]
satisfying the computation rule $\ind_\Sigma (g,\pair(x,y)) \doteq g(x,y)$.
\end{defn}

It is common to write ``$(x,y)$'' as shorthand for ``$\pair(x,y)$.''

\begin{defn} Given a type family $x :A \vdash B(x) \univ$ by the induction principle for $\Sigma$-types, we have a function
\[ \pr_1 : \sum_{x :A}B(x) \to A\]
defined by $\pr_1(x,y) \coloneq x$ and a dependent function
\[ \pr_2 :  \prod_{ p : \sum_{x :A}B(x)} B(\pr_1(p))\]
defined by $\pr_2(x,y) \coloneq y$.
\end{defn}

When $B$ is a constant type family over $A$, the type $\sum_{x : A}B$ is the type of ordinary pairs $(x,y)$ where $x :A $ and $y: B$. Thus \textbf{product types} arise as special cases of $\Sigma$-types.

\begin{defn} Given types $A$ and $B$ their product type is the type $A \times B \coloneq \sum_{x :A}B$. It comes with a pairing function
\[ (-,-) : A \to B \to A \times B\] and satisfies an induction principle:
\[ \ind_\times : \prod_{x : A} \prod_{y : B} P(x,y) \to \prod_{z : A \times B} P(z)\]
satisfying the computation rule $\ind_\times (g,(x,y)) \doteq g(x,y)$.
\end{defn}

As a special case, we have
\[ \ind_\times : (A \to B \to C) \to ((A \times B) \to C).\]
This is the inverse of the \textbf{currying function}. Thus $\ind_\times$ and $\ind_\Sigma$ sometimes go by the name \textbf{uncurrying}.

\section*{September 15: Identity types}

We have started to develop an analogy in which types play the role of mathematical propositions and terms in a type play the role of proofs of that proposition. More exactly, we might think of a type as a ``proof-relevant'' proposition, the distinction being that the individual proofs of a given proposition---the terms of the type---are first class mathematical objects, which may be used as ingredients in future proofs, rather than mere witnesses to the truth of the particular proposition.

The various constructions on types that we have discussed are analogous to the logical operations ``and,'' ``or,'' ``implies,'' ``not,'' ``there exists,'' and ``for all.'' We also have the unit type $\1$ to represent the proposition $\top$ and the empty type $\emptyset$ to represent the proposition $\bot$. There is one further ingredient from first-order logic that is missing a counterpart in dependent type theory: the logical operation ``$=$.''

Given a type $A$ and two terms $x,y : A$ it is sensible to ask whether $x = y$. From the point of view of types as proof-relevant propositions, ``$x=y$'' should be the name of a type, in fact a dependent type. The formation rule for \textbf{identity types} says
\[
\inferrule{ \Gamma \vdash A \univ}
{\Gamma, x : A, y : A \vdash x =_A y \univ}
\]
where ``$x=y$'' is commonly used as an abbreviation for ``$x=_Ay$'' when the type of $x$ and $y$ is clear from context. A term $p : x = y$ of an identity type is called an \textbf{identification} of $x$ and $y$ or a \textbf{path} from $x$ to $y$ (more about this second term later). Identifications have a rich structure that follows from a very simple characterization of the identity type due to Per Martin-L\"{o}f: it is the inductive type family freely generated by the reflexivity terms.

\subsection*{The inductive definition of identity types}

We can define identity types as inductive types in either a one-sided or two-sided fashion. The induction rule may be easier to understand from the one-sided point of view, so we present it first.

\begin{defn}[one-sided identity types] Given a type $A$ and a term $a : A$, the \textbf{identity type} of $A$ at $a$ is the inductive family of types $x : A \vdash a =_A x \univ$ with a single constructor $\refl_a : a =_A a$. The induction principle is postulates that for any type family $x : A, p : a =_A x \vdash P(x,p) \univ$ there is a function
\[ \pathind_a : P(a, \refl_a) \to \prod_{x : A} \prod_{p : a =_A x} P(x,p)\]
satisfying $\pathind_a(q,a, \refl_a) \doteq q$.
\end{defn}

This is a very strong induction principle: it says that to prove a predicate $P(x,p)$ depending on any term $x :A$ and any identification $p : a =_A x$ it suffices to assume $x$ is $a$ and $p$ is $\refl_a$ and prove $P(a,\refl_a)$. 

More formally, identity types are defined by the following rules:
\[
\inferrule{ \Gamma \vdash a : A}{ \Gamma, x : A \vdash a =_A x \univ} \qquad
\inferrule{\Gamma \vdash a : A}{\Gamma \vdash \refl_a : a =_A a}\]
\[ 
\inferrule{\Gamma \vdash a : A \\ \Gamma, x : A, p: a=_A x \vdash P(x,p) \univ}{\Gamma \vdash \pathind_a : P(a,\refl_a) \to \prod_{x :A}\prod_{p : a =_A x}P(x,p)} \qquad
\inferrule{\Gamma \vdash a : A \\ \Gamma, x : A, p: a=_A x \vdash P(x,p) \univ}{\Gamma \vdash \pathind_a(q,a, \refl_a) \doteq q : P(a,\refl_a)}
\]

Equally, the identity type can be considered in a two-sided fashion:
\begin{defn}[two-sided identity types]
 Given a type $A$, the \textbf{identity type} of $A$ is the inductive family of types $x : A, y :A \vdash x =_A y \univ$ with a single constructor $x : A \vdash \refl_x : x =_A x$. The induction principle is postulates that for any type family $x : A, y : A, p : x =_A y \vdash P(x,y,p) \univ$ there is a function
\[ \pathind : \prod_{a : A} P(a, a, \refl_a) \to \prod_{x : A} \prod_{y: A}\prod_{p : x =_A y} P(x,y,p)\]
satisfying $\pathind(q,a, a,\refl_a) \doteq q$.
\end{defn}

In this form, the identity types are defined by the following rules:
\[
\inferrule{ \Gamma \vdash A}{ \Gamma, x : A, y : A \vdash x =_A y \univ} \qquad
\inferrule{\Gamma \vdash A}{\Gamma, x :A \vdash \refl_x : x =_A x}\]
\[ 
\inferrule{\Gamma \vdash A \\ \Gamma, x : A, y : A, p: x=_A y \vdash P(x,y,p) \univ}{\Gamma \vdash \pathind : \prod_{a : A} P(a,a,\refl_a) \to \prod_{x :A}\prod_{y :A}\prod_{p : x =_A y}P(x,y, p)} \quad
\inferrule{\Gamma \vdash  A \\ \Gamma, x : A, y : A, p: x=_A y \vdash P(x,y,p) \univ}{\Gamma, a : A \vdash  \pathind (q,a,a,\refl_a) \doteq q : P(a,a,\refl_a)}
\]

These presentations are interderivable.

\subsection*{The groupoid structure on types}

Mathematical equality, as traditionally understood, is an equivalence relation: it's reflexive, symmetric, and transitive. But all we've asserted about identity types is that they are inductively generated by the reflexivity terms! As we'll now start to discover, considerable additional structure follows.

\begin{prop}[symmetry] For any type $A$, three is an  \textbf{inverse operation}
\[ \inv : \prod_{x, y :A} x = y \to y =x.\]
\end{prop}
\begin{proof}
We define $\inv$ by path induction. By the introduction rule for function types it suffices to define $\inv p : y =x$ for $p : x = y$. Consider the type family $x : A, y : A, p: x = y \vdash P(x,y,p) \coloneq y = x$. By path induction to inhabit $y=x$ it suffices to assume $x = y$ and $p$ is $\refl_x$ in which case we may define $\inv \refl_x \coloneq \refl_x : x=x$. Thus $\inv$ is
\[ \pathind (\lambda x, \refl x) : \prod_{x :A} \prod_{y :A} \prod_{x = y} y =x.\]
\end{proof}

\begin{ntn} Write $p^{-1}$ for $\inv(p)$.
\end{ntn}

\begin{prop}[transitivity]
For any type $A$, there is a  \textbf{concatenation} operation
\[ \concat : \prod_{x,y,z :A} x= y \to y=z \to x = z.\] 
\end{prop}
\begin{proof}
We define $\concat$ by appealing to the path induction principle for identity types. 
By the introduction rule for dependent function types, to define $\concat$ you may assume given $p : x=y$. The task is then to define $\concat (p) : \prod_{z} y =z \to x = z$.  For this, consider the type family $x :A , y : A, p : x = y \vdash P(x,y,p)$ where $P(x,y,p) \coloneq \prod_{z : A} (y = z) \to (x = z)$. By applying the function $\pathind$ to get a term of this type it suffices to assume $y$ is $x$ and $p$ is $\refl_x$. So we need only define $\concat( \refl_x) : \prod_{z :A} x =z \to x = z$ and we define this to be the identity function $\id_{x = z}$. Thus the function $\concat$ is 
\[  \pathind( \lambda x, \lambda z, \id_{x=z}) : \prod_{x :A}\prod_{y :A} \prod_{p : x = y} \prod_{z :A} y=z \to x =z,\]
which can be regarded as a function in the type $\prod_{x,y,z :A} x= y \to y=z \to x = z$ by swapping the order of the arguments $p$ and $z$.
\end{proof}

\begin{ntn} Write $p \cdot q$ for $\concat(p,q)$.
\end{ntn}

While the elimination rule for identity types is quite strong the corresponding computation rule is relatively weak. It's not strong enough to show that $(p \cdot q) \cdot r$ and $p \cdot (q \cdot r)$ are judgmentally equal for any $p : x = y$, $q : y = z$, and $r : z = q$. In fact there are countermodels that show that this is false in general. However, since both $(p \cdot q) \cdot r$ and $p \cdot (q \cdot r)$ are terms of type $x = w$ we can ask whether there is an identification between them and it turns out this is always true.

\begin{prop}[associativity] Given $x,y,z,w  :A$ and identifications $p : x = y$, $q : y =z$, and $r : z = q$, there is an associator
\[ \assoc (p,q,r) : (p \cdot q) \cdot r = p \cdot (q \cdot r)\]
\end{prop}
\begin{proof} We define $\assoc(p,q,r)$ by path induction. 

Consider the type family $x :A, y : A, p: x = y \vdash \prod_{z :A} \prod_{q : y =z} \prod_{w:A} \prod_{r : z =w} (p \cdot q) \cdot r = p \cdot (q \cdot r)$. To define a term $\assoc (p,q,r)$ in here it suffices to assume $y$ is $x$ and $p$ is $\refl_x$ and define
\[ \lambda z. \lambda q. \lambda w. \lambda r. \assoc(\refl_x, q,r) : \prod_{z :A} \prod_{q : x =z} \prod_{w:A} \prod_{r : z =w} (\refl_x \cdot q) \cdot r = \refl_x \cdot (q \cdot r).\]
By the definition of concatenation, $\refl_x \cdot q \doteq q$ and $\refl_x \cdot (q \cdot r) \doteq q \cdot r$. So we must define 
\[ \assoc(\refl_x, q,r) :  q \cdot r = q \cdot r\]
and we can take this term to be $\refl_{q \cdot r}$.
\end{proof}

\begin{prop}[units] For any type $A$, there are left and right \textbf{unit laws}
\[ \lambda x. \lambda y. \lambda p. \term{left-unit}(p) :  \lambda {x,y : A} \prod_{p: x = y} \refl_x \cdot p = p \qquad  \lambda x. \lambda y. \lambda p.\term{right-unit}(p) : \prod_{x , y : A} \prod_{p : x = y} p \cdot \refl_y = p.\]
\end{prop}
\begin{proof}
We are asked to define  dependent functions that takes $x, y :A$ and $p : x = y$ and produce terms
\[ \term{left-unit}(p) : \refl_x \cdot p = p \qquad \term{right-unit}(p) : p \cdot \refl_y = p.\]
By path induction, it suffices to assume $y$ is $x$ and $p$ is $\refl_x$, in which case we require terms
\[ \term{left-unit}(\refl_x) : \refl_x \cdot \refl_x = \refl_x \qquad \term{right-unit}(\refl_x) : \refl_x \cdot \refl_x = \refl_x.\] By the definition of concatenation $\refl_x \cdot \refl_x \doteq \refl_x$ so we can take $\refl_{\refl_x}$ as both $\term{left-unit}(\refl_x)$ and $\term{right-unit}(\refl_x)$.
\end{proof}

\begin{prop}[inverses] For any type $A$, there are left and right \textbf{inverse laws}
\[  \lambda x. \lambda y. \lambda p.\term{left-inv}(p) : \prod_{x,y :A} \prod_{p : x= y} p^{-1} \cdot p = \refl_y \qquad  \lambda x. \lambda y. \lambda p.\term{right-inv}(p) : \prod_{x,y :A} \prod_{p : x=y} p \cdot p^{-1} = \refl_x.\]
\end{prop}
\begin{proof}
We are asked to define  dependent functions that takes $x, y :A$ and $p : x = y$ and produce terms
\[ \term{left-inv}(p) : p^{-1} \cdot p = \refl_y \qquad \term{right-inv}(p) : p \cdot p^{-1} = \refl_x.\]
By path induction, it suffices to assume $y$ is $x$ and $p$ is $\refl_x$, in which case we require terms
\[ \term{left-inv}(\refl_x) : \refl_x^{-1} \cdot \refl_x = \refl_x \qquad \term{right-inv}(\refl_x) : \refl_x \cdot \refl_x^{-1} = \refl_x.\]
By the definitions of concatenation and inverses, again both left-hand and right-hand sides are judgementally equal so we take $\term{left-inv}(\refl_x)$ and $\term{right-inv}(\refl_x)$ to be $\refl_{\refl_x}$.
\end{proof}

\section*{September 20: More identity types}

\subsection*{Types as \texorpdfstring{$\infty$}{infinity}-groupoids}

Martin-L\"{o}f's rules for the identity types date from a 1975 paper ``An Intuitionistic Theory of Types.'' In the following two decades, there was a conjecture that went by the name ``uniqueness of identity proofs" that for any $x,y : A$, $p, q  : x =_A y$, the type $p =_{x=_A y} q$ is inhabited, meaning that it's possible to construct an identification between $p$ and $q$. In 1994, Martin Hofmann and Thomas Streicher constructed a model of Martin-L\"{o}f's dependent type theory in the category of groupoids that refutes uniqueness of identity proofs.\footnote{The technical details of what exactly it means to ``construct a model of type theory'' are quite elaborate and would be interesting to explore as a final project.}

In the Hofmann-Streicher model, types $A$ correspond to \emph{groupoids} and terms $x, y : A$ correspond to \emph{objects} in the groupoid. An identification $p : x = y$ corresponds to a(n iso)morphism $p : x \to y$ in the groupoid, while an identification between identifications exists if and only if $p$ and $q$ define the same morphism. Since there are groupoids with multiple distinct morphisms between a fixed pair of objects, we see that it is not always the case that $p=_{x=_A y} q$. Following Hofmann-Streicher, it made sense to start viewing types as more akin to groupoids than to sets. The proofs of symmetry and transitivity for identity types are more accurately described as inverses and concatenation operations in a groupoid. As we've seen, these satisfy various associativity, unit, and inverse laws---up to identification at least---as required by a groupoid.

But that last caveat is important. We've shown that for any type $A$, its identity types $x,y :A \vdash x=_A y \univ$ give it something like the structure of a groupoid. But for each $x,y :A$, $x=_A y$ is also a type, so \emph{its} identity types $p,q: x=_A y \vdash p =_{x=_A y}q \univ$ give $x=_A y$ its own groupoid structure. And the higher identity types, $\alpha, \beta : p =_{x=_A y}q  \vdash \alpha= \beta \univ$ give $p =_{x=_A y}q$ its own groupoid structure and so on. So a modern point of view is that the types in Martin-L\"{o}f's dependent type theory should be thought of as $\infty$-\emph{groupoids}.

If $A$ is an $\infty$-groupoid, its terms $x :A$ might be called \textbf{points} and its identifications $p : x =_A y$ might be called \textbf{paths}. This explains the modern name ``path induction'' for the induction principle for identity types. These ideas are at the heart of the homotopical interpretation of type theory, about more which later.

\subsection*{The uniqueness of \texorpdfstring{$\refl$}{refl}}

The definition of the identity types says that the family of types $a =x$ indexed by $x : A$ is inductively generated by the term $\refl_a : a = a$. It does \emph{not} say that the type $a =a$ is inductively generated by $a : A$. In particular, we cannot apply path induction to prove that $p = \refl_a$ for any $p : a = a$ because in this case neither endpoint of the identity type is free. 

There is a sense however in which the reflexivity term is unique:

\begin{prop} For any type $A$ and $a : A$, $(a, \refl_a)$ is the unique term of the type $\sum_{x : A} a =x$. That is, for any $z : \sum_{x :A} a =x$, there is an identification $(a, \refl_a) = z$.
\end{prop}
\begin{proof} We're trying to define a dependent function that takes $z : \sum_{x :A} a =x$ and gives a term in the identity type $(a,\refl_a) =_{\sum_{x :A} a =x} z$. By $\Sigma$-induction it suffices to assume $z$ is a pair $(x,p)$ where $x :A$ and $p : a =x$ and construct an identification $(a, \refl_a) =_{\sum_{x :A} a =x} (x,p)$. So now we're trying to define a dependent function that takes $x :A$ and $p : a =x$ and constructs an identification $(a, \refl_a) =_{\sum_{x :A} a =x} (x,p)$. By path induction, it suffices to assume $x$ is $a$ and $p$ is $\refl_a$. But now we can use reflexivity to show that $(a, \refl_a) = (a, \refl_a)$.
\end{proof}

In terminology to be introduced later, this result says that the type $\sum_{x : A} a =x$ is \textbf{contractible} with the term  $(a, \refl_a)$ serving as its \textbf{center of contraction}.


\subsection*{The action of paths on functions}

The structural rules of type theory guarantee that any function (and indeed any construction in type theory) preserve definitional equality. We now show that in addition every function preserves identifications.

\begin{prop} Let $f \colon A \to B$. There is an operation that defines the \textbf{action on paths} of $f$
\[ \ap_f : \prod_{x,y :A} (x=y) \to (f(x) = f(y))\]
that satisfies the coherence conditions
\[ \apcoh{id}_A : \prod_{x,y:A} \prod_{p : x= y} p = \ap_{\id_A}(p)\]
\[ \apcoh{comp}(f,g) : \prod_{x,y:A} \prod_{p: x=y} \ap_g(\ap_f(p))= \ap_{g\circ f}(p).\]
\end{prop}
\begin{proof}
By path induction to define $\ap_f(p) : f(x)=f(y)$ it suffices to assume $y$ is $x$ and $p$ is $\refl_x$. We may then define $\ap_f(\refl_x) \coloneq \refl_{f(x)} : f(x) = f(x)$.

Next to define $\apcoh{id}_A$ it similarly suffices to suppose $y$ is $x$ and $p$ is $\refl_x$. Since $\ap_{\id_A}(\refl_x)\doteq \refl_x$, we may define $\apcoh{id}_A(\refl_x) \coloneq \refl_{\refl_x} : \refl_x = \refl_x$.

Finally, to define $\apcoh{comp}(f,g)$, by path induction we may again assume $y$ is $x$ and $p$ is $\refl_x$. Since both $\ap_g(\ap_f(\refl_x))$ and $\ap_{g \circ f}(\refl_x)$ are defined to be $\refl_{g(f(x))}$ we may define $\apcoh{comp}(f,g)(\refl_x)$ to be $\refl_{\refl_{g(f(x))}}$.
\end{proof}

If the types $A$ and $B$ are thought of as $\infty$-groupoids, then $f \colon A \to B$ can be thought of as a functor of $\infty$-groupoids in a sense hinted at by the following lemma.

\begin{lem} For $f \colon A \to B$ there are identifications
\begin{align*}
\apcoh{refl}(f,x) &: \ap_f (\refl_x) = \refl_{f(x)} \\
\apcoh{inv}(f,p) &: \ap_f (p^{-1}) = \ap_f(p)^{-1} \\
\apcoh{concat}(f,p,q) &: \ap_f(p \cdot q) = \ap_f(p) \cdot \ap_f(q)
\end{align*}
for every $p : x= y$ and $q: y = z$.
\end{lem}
\begin{proof}
For the first coherence, there is a definitional equality $\ap_f (\refl_x) \doteq \refl_{f(x)}$ so we take $\apcoh{refl}(f,x) \coloneq \refl_{\refl_{f(x)}}$. 

We define $\apcoh{inv}(f,p)$ by path induction on $p$ by defining $\apcoh{inv}(f,\refl_x) \coloneq \refl_{\refl_{f(x)}}$.

Similarly, we define $\apcoh{concat}(f,p,q)$ by path induction on $p$ (since concat was defined by path induction on $p$) by defining $\apcoh{concat}(f,\refl_x,q)$ to be $\refl_{\ap_f(q)}$.
\end{proof}

\subsection*{Transport}

The term $\ap_f$ defines the action of a non-dependent function $f \colon A \to B$ on paths in $A$. It's natural to ask whether a dependent function $f : \prod_{z:A}B(z)$ also induces an action on paths. There's a challenge here, though. If $x,y : A$ are terms belonging to the base type, then we can form the type $x=_A y$ to ask whether they are identifiable. But the terms $f(x) : B(x)$ and $f(y) : B(y)$  belong to different types and are not identifiable. But nevertheless if there is  path $p : x = y$ identifying $y$ with $x$ intuition suggests there should be some way to compare $f(y)$ to $f(x)$.

To achieve this, we must construct a different sort of action of paths function first. This is called the  \textbf{transport} function for dependent types $x :A \vdash B(x) \univ$ that, given an identification $p : x = y$ in the base type, can be used to transport any term in $B(x)$ to a term in $B(y)$.

\begin{prop} For any type family $x : A \vdash B(x) \univ$, there is a \textbf{transport operation}
\[ \tr_B : \prod_{x,y :A} (x=y) \to (B(x) \to B(y)).\]
\end{prop}
\begin{proof}
By path induction it suffices to define $\tr_B(\refl_x)) \coloneq \id_{B(x)}$.
\end{proof}

As an application of transport we can now defined the action on paths of a dependent function.

\begin{prop} For any dependent function $f : \prod_{z: A} B(z)$ and identification $p : x=_A y$ there is a path
\[ \apd_f(p) : \tr_B(p, f(x)) =_{B(y)} f(y).\]
\end{prop}
\begin{proof}
The function
 \[ \lambda x . \lambda y. \lambda p. \apd_f(p) : \prod_{x,y:A} \prod_{p : x=y} \tr_B(p, f(x)) =_{B(y)} f(y)\]
 may be defined by path induction on $p$. It suffices to construct a path
 \[ \lambda x . \apd_f(\refl_x) : \prod_{x :A} \tr_B(\refl_x,f(x)) =_{B(x)} f(x).\]
Since  $\tr_B(\refl_x),f(x)) \doteq f(x)$ we may defined $\apd_f(\refl_x) \coloneq \refl_{f(x)}$.
\end{proof}

\subsection*{The laws of addition on $\bN$}

Recall that we defined the addition of natural numbers in such a way that
\[ m+ 0 \doteq m \qquad m+ \suc(n) \doteq \suc(m+n)\]
by induction on the second variable. With this definition, these are the only definitional equalities. However, it is possible to produce identifications proving the other commutative monoid axioms.

\begin{lem} For any $n : \bN$ there are identifications
\[\term{left-unit-law-add}_\bN(n) : 0 + n = n \qquad \term{right-unit-law-add}_\bN(n) : n+0=n.\]
\end{lem}
\begin{proof}
The second of these can be taken to be $\refl_n$ but the first is more complicated. We define $\term{left-unit-law-add}_\bN(n)$ by induction on $n : \bN$. When $n = 0$, $0+0=0$ holds by reflexivity. 

Our final goal is to show $0+\suc(n) = \suc(n)$, for which it suffices to construct an identification \[ \suc(0+n) = \suc(n)\]
by the definition of addition. We may assume we have an identification $p : 0 + n = n$. Thus, we can use the action on paths of $\suc : \bN \to \bN$  to obtain a term $\ap_{\suc}(p) : \suc(0+n) = \suc(n)$.
\end{proof}

\begin{prop} For any $m,n : \bN$ there are identifications
\begin{align*} \term{left-successor-law-add}_\bN(m,n) &: \suc(m) + n = \suc(m+n) \\
\term{right-sucessor-law-add}_\bN(m,n) &= m+ \suc(n) = \suc(m+n)
\end{align*}
\end{prop}
\begin{proof}
Again the second identification holds judgmentally so we  define \[\term{right-sucessor-law-add}_\bN(m,n) \coloneq \refl_{\suc(m+n)}.\] We construct the former using induction on $n \in \bN$. The base case $\suc(m)+0 = \suc(m+0)$ holds by $\refl_{\suc(m)}$. For the inductive step we assume we have an identification $p : \suc(m)+n = \suc(m+n)$. Our goal is to show that $\suc(m)+\suc(n) = \suc(m+\suc(n))$. By action of paths of $\suc : \bN \to \bN$ we obtain a term
\[ \ap_{\suc}(p) : \suc(\suc(m)+n) = \suc(\suc(m+n))\] but here the left hand side is judgmentally equal to $\suc(m)+\suc(n)$ while the right hand side is judgmentally equal to $\suc(m+ \suc(n))$.
\end{proof}  

\begin{prop}[associativity] For all $k,m,n : \bN$,
\[ \term{associative-add}_\bN(k,m,n) : (m+n)+k = m+(n+k).\]
\end{prop}
\begin{proof}
We construct $\term{associative-add}_\bN(k,m,n)$ by induction on $n$. In the base case we have
\[ (k+m)+0 \doteq k+m \doteq k + (m+0),\] so we define $\term{associative-add}_\bN(k,m,0) \coloneq \refl_{m+n}$.

For the inductive step let $p : (k+m)+n = k+(m+n)$. We then have
\[ \ap_{\suc}(p) : \suc((k+m)+n) = \suc(k+(m+n)).\]
We have $\suc((k+m)+n) \doteq (k+m)+\suc(n)$ and $\suc(k+(m+n)) \doteq k + \suc(m+n) \doteq k + (m + \suc(n))$ so this term is the term we wanted.
\end{proof}


\begin{prop}[commutativity] For all $m,n : \bN$,
\[ \term{commutative-add}_\bN(m,n) : m+n = n+m.\]
\end{prop}
\begin{proof} By induction on $m$ we have to show $0+n = n+0$, which holds by the unit laws for $n$. Then we may assume $p : m+n = n+m$ and must show $\suc(m)+n= n + \suc(m)$. We have
\[ \ap_{\suc}(p) : \suc(m+n) = \suc(n+m).\] We then concatenate this path with the paths $ \term{left-successor-law-add}_\bN(m,n)$ and $\term{right-successor-law-add}_\bN(n,m)$ to obtain the identification we want. 
\end{proof}

\section*{September 22: Universes}

Recall that in Martin-L\"{o}f's dependent type theory, $\bN$ was defined as the inductive type freely generated by a term $0_\bN : \bN$ and a function $\suc :\bN \to \bN$. The corresponding induction principle gives a strengthened version of the Dedekind-Peano principle of mathematical induction, but two of the traditional axioms---namely that $0_\bN$ is not a successor and $\suc$ is injective---are missing. Using our type forming operations, we can define the types that assert those axioms:
\[ \prod_{n : \bN} (n = 0_\bN) \to \emptyset \qquad \prod_{n,m : \bN} (\suc(n) = \suc(m)) \to (n = m)\]
but we don't yet have the tools needed to construct terms in those types. Type theoretic \emph{universes} will enable us to construct terms in these types and prove many other things besides.

Informally, a universe $\UU$ can be thought of as a ``type whose terms are types.'' More precisely, a universe is a type $\UU$ together with a type family $X : \UU \vdash \sT(X)$ called the \emph{universal type family}. We think of the term $X$ as an \emph{encoding} of the type $\sT(X)$ though its common to conflate these notions notationally, writing ``$X$'' for both the encoding and the type.

Universes are assumed to be closed under all the type constructors in a sense to be made precise below. To avoid a famous inconsistency, however, we do not assume that the universe is contained in itself. One way to think about this is that $\UU$ is the type of ``small'' types, but $\UU$ itself is not ``small.''

In the presence of a universe $\UU$, a family of small types $x : A \vdash B(x) \univ$ over a type $A$ can be encoded by a function $B \colon A \to \UU$ defined by sending the term $x$ to the encoding of the type $B(x)$.\footnote{This is already how we have been defining type families in \texttt{agda}.} In particular, if $A$ is an inductive type, freely generated by some finite list of constructors, then \emph{type families} over $A$---not just dependent functions over $A$---can be defined inductively by specifying types for each of the constructors. We will see examples of this soon.

\subsection*{Type theoretic universes}

\begin{defn} A \textbf{universe} is a type $\UU$ in the empty context equipped with a type family $X : \UU \vdash \sT(X) \univ$ over $\UU$ called the \textbf{universal family of types} that is closed under the type forming operations in the sense that it is equipped with the following structure:
\begin{enumerate}
\item $\UU$ contains terms $\check{\emptyset}$, $\check{\1}$, $\check{\bN}$ that satisfy the judgmental equalities
\[ \sT(\check{\emptyset}) \doteq \emptyset, \quad \sT(\check{\1}) \doteq \1, \quad \sT(\check{\bN}) \doteq \bN.\]
\item $\UU$ is closed under coproducts in the sense that it comes equipped with a function
\[ \check{+} \colon \UU \to \UU \to \UU\] that satisfies $\sT(X\check{+}Y) \doteq \sT(X) + \sT(Y)$.
\item $\UU$ is closed under $\Pi$-types in the sense that it comes equipped with a function
\[ \check{\Pi} \colon \prod_{X: \UU} (\sT(X) \to \UU) \to \UU\] satisfying
\[ \sT(\check{\Pi}(X,P)) \doteq \prod_{x : \sT(X)} \sT(P(x))\]
for all $X : \UU$ and $P \colon \sT(X) \to \UU$.
\item $\UU$ is closed under $\Sigma$-types in the sense that it comes equipped with a function
\[ \check{\Sigma} \colon \prod_{X: \UU} (\sT(X) \to \UU) \to \UU\] satisfying
\[ \sT(\check{\Sigma}(X,P)) \doteq \sum_{x : \sT(X)} \sT(P(x))\]
for all $X : \UU$ and $P \colon \sT(X) \to \UU$.
\item $\UU$ is closed under identity types in the sense that it comes equipped with a function
\[ \check{\Id} : \prod_{X : \UU} \sT(X) \to \sT(X) \to \UU\]
satisfying
\[ \sT(\Id(X,x,y)) \doteq (x = y)\]
for all $X: \UU$ and $x,y : \sT(X)$.
\end{enumerate}
\end{defn} 

\begin{defn} Given a universe $\UU$, we say a type $A$ in context $\Gamma$ is \textbf{small} if it occurs in the universe: i.e., if it comes equipped with a term $\check{A} : \UU$ in context $\Gamma$ for which the judgment
\[ \Gamma \vdash \sT(\check{A}) \doteq A \univ\]
holds. 
\end{defn}

When $A$ is a small type, it's common to write $A$ for both $\check{A}$ and $\sT(A)$. So by $A : \UU$ we mean that $A$ is a small type.

\subsection*{Assuming enough universes}

Most of the time it's sufficient to assume just one universe $\UU$. But on occasion, it is useful to assume that $\UU$ itself is a type in some universe. 

\begin{post} We assume that there are \textbf{enough universes}, i.e., that for every finite list of types in context
\[ \Gamma_1 \vdash A_1 \univ \quad \cdots \quad \Gamma_n \vdash A_n \univ\]
there is a universe $\UU$ that contains each $A_i$ in the sense that $\UU$ has terms
\[ \Gamma_i \vdash \check{A}_i : \UU\] for which $\Gamma_i \vdash \sT(\check{A}_i) \doteq A_i \univ$ holds. 
\end{post}

With this assumption it's rarely necessary to work with more than one universe at the same time. 

As a consequence of our postulate that there exist enough universes, we obtain specific universes:\footnote{In \texttt{agda}, this structure is formalized in the file Agda.Primitive.}

\begin{defn} The \textbf{base universe} $\UU_0$ is obtained by applying the postulate to the empty list of types in context.
\end{defn}

\begin{defn} The \textbf{successor universe} of any universe $\UU$ is the universe $\UU^+$ obtained from the finite list
\[ \vdash \UU \univ \quad X: \UU \vdash \sT(X) \univ\]
\end{defn}
Thus the successor universe contains both $\UU$ and any type in $\UU$. 

\begin{defn} The \textbf{join} of two universes $\UU$ and $\mathcal{V}$ is the universe $\UU \sqcup \mathcal{V}$ obtained by applying the postulate to the type families
\[ X : \UU \vdash \sT_\UU(X) \univ \qquad Y : \mathcal{V} \vdash \sT_{\mathcal{V}}(Y) \univ\]
\end{defn}

\subsection*{Observational equality on \texorpdfstring{$\bN$}{N}}

To illustrate what universes are for, we define a type family $m : \bN, n : \bN \vdash \Eq_\bN(m,n) \univ$ that we call \textbf{observational equality} on $\bN$. Because type families can now be thought of as functions $\Eq_\bN : \bN \to \bN \to \UU$ we can use the induction principle of $\bN$ to define this type family.  We'll then prove that $\Eq_\bN$ is \textbf{logically equivalent} to the identity type family; in fact, we'll later see that these types are \textbf{equivalent}, once we know what that means. The advantage of the type family $\Eq_\bN$ is that it's characterized more explicitly, so this will help us prove theorems about the identity type family over the natural numbers.

\begin{defn} We define \textbf{observational equality} of $\bN$ as the type family $\Eq_\bN : \bN \to \bN \to \UU$ satisfying
\[ \Eq_\bN(0_\bN,0_\bN) \doteq \1 \quad \Eq_\bN(\suc(n),0_\bN) \doteq \emptyset \quad \Eq_\bN(0,\suc(n)) \doteq \emptyset \quad \Eq_\bN(\suc(m), \suc(n)) \doteq \Eq_\bN(m,n).\]
\end{defn}

\begin{lem} Observational equality on $\bN$ is reflexive:
\[ \refl-\Eq_\bN : \prod_{n : \bN} \Eq_\bN(n,n).\]
\end{lem}
\begin{proof}
We define $\refl-\Eq_\bN$ by induction by $\refl-\Eq_\bN(0_\bN) \coloneq \star$ and $\refl-\Eq_\bN(\suc(n)) \coloneq \refl-\Eq_\bN(n)$.
\end{proof}

\begin{prop} For any $m,n : \bN$, the types $\Eq_\bN(m,n)$ and $(m=n)$ are \textbf{logically equivalent}: that is there are functions
\[ (m = n) \to \Eq_\bN(m,n) \quad \text{and} \quad \Eq_\bN(m,n) \to (m=n).\]
\end{prop}
\begin{proof}
By path induction, there is a function $\term{id-to-eq} : \prod_{m,n: \bN} (m =n) \to \Eq_\bN(m,n)$ defined by $\term{id-to-eq}(n,\refl_n) \coloneq \refl-\Eq_\bN(n)$. 

For the converse, we define a function $\term{eq-to-id} : \prod_{m,n : \bN} \Eq_\bN(m,n) \to (m=n)$ by induction on $m$ and $n$. We define $\term{eq-to-id}(0_\bN,0_\bN) : \Eq_\bN(0_\bN,0_\bN) \to (0_\bN = 0_\bN)$, by induction on $\Eq_\bN(0_\bN,0_\bN) \doteq \1$ to be the function that sends $\star : \1$ to $\refl_{0_\bN} : 0_\bN = 0_\bN$. We define the functions $\term{eq-to-id}(\suc(n),0_\bN)$ and $\term{eq-to-id}(0_\bN,\suc(n))$ using $\term{ex-falso}$, since both of these are maps out of the empty type. Finally, to define $\term{eq-to-id}(\suc(m),\suc(n))$ we may use a function $f : \Eq_\bN(m,n) \to (m=n)$, in which case, $\term{eq-to-id}(\suc(m),\suc(n))$  is defined to be the composite function
\[ \Eq_\bN(\suc(m),\suc(n)) \xrightarrow{\id} \Eq(m,n) \xrightarrow{f} (m=n) \xrightarrow{\ap_{\suc}} (\suc(m)=\suc(n)).\]
\end{proof}

\begin{ntn} For types $A$ and $B$, we write $A \leftrightarrow B$ as an abbreviation for the type
\[ (A \to B) \times (B \to A).\]
\end{ntn}

Thus the logical equivalence defines a term in the type
\[ \prod_{m,n : \bN} \Eq_\bN(m,n) \leftrightarrow (m=n).\]

\subsection*{Peano's axioms}

\begin{thm} For any $m,n : \bN$ we have
\[ (m=n) \leftrightarrow (\suc(m) = \suc(n))\]
\end{thm}
\begin{proof}
The action of paths of the successor function proves the forwards implication
\[ \ap_{\suc} : (m=n) \to (\suc(m) = \suc(n))\]
The direction of interest is the converse which proves that successor is injective.

Using the logical equivalences $(m=n) \leftrightarrow \Eq_\bN(m,n)$ we define the reverse implication to be the composite
\[ (\suc(m) = \suc(n)) \xrightarrow{\term{id-to-eq}(\suc(m),\suc(n))} 
\Eq_\bN(\suc(m),\suc(n)) \xrightarrow{\id} \Eq_\bN(m,n) \xrightarrow{\term{eq-to-id}(m,n)} (m=n).\]
\end{proof}

\begin{thm} For any $n : \bN$, $\neg(0_\bN  = \succ_\bN(n))$.
\end{thm}
\begin{proof}
We have a family of maps
\[ \lambda n, \term{id-to-eq}(0_\bN,n) : \prod_{n : \bN} (0_\bN = n) \to \Eq_\bN(0_\bN,n).\]
Since $\Eq_\bN(0_\bN, \suc(n)) \doteq \emptyset$ we have
\[ \term{id-to-eq}(0_\bN,\suc(n)) : (0_\bN = \suc(n)) \to \emptyset\]
which is precisely the claim.
\end{proof}

\section*{September 27: Modular arithmetic}

Having fully described Martin-L\"{o}f's dependent type theory, we may now start developing some mathematics in it. The fundamental idea used to develop mathematics is something we've already previewed: the Curry-Howard interpretation.

\subsection*{The Curry-Howard interpretation}

The Curry-Howard interpretation is an interpretation of logic into type theory. In type theory, there is no separation between the logical framework and the general theory of collections of mathematical objects the way there is in the more traditional setup with Zermelo-Fraenkel set theory, which is postulated by axioms in first order logic. The idea is that propositions may be expressed as types with proofs of those propositions expressed as terms in those types. For example:

\begin{defn} We say that a natural number $d$ divides a natural number $n$ if there is a term in the type
\[ d \mid n \coloneq \sum_{k : \bN} d \cdot k = n \]
defined using the multiplication $\cdot$ on $\bN$, the identity type of $\bN$, and the dependent sum of the type family $k : \bN \vdash d \cdot k = n \univ$.
\end{defn}

Just as existential quantification $(\exists)$ is expressed using $\Sigma$-types, universal quantification $(\forall)$ is expressed using $\Pi$-types. For example, the type
\[ \prod_{n : \bN} 1 \mid n\]
asserts that every natural number is divisible by 1. The term
\[ \lambda n. (n, \term{left-unit}(n)) : \prod_{n : \bN} 1 \mid n\]
proves this result.

\begin{prop} Let $d, m,n : \bN$. If $d$ divides any two of $m$, $n$, and $m+n$, then $d$ divides the third.
\end{prop}
\begin{proof}
We prove only that if $d \mid m$ and $d \mid n$ then $d \mid m+n$.
By hypothesis we have terms:
\[ H : \sum_{k : \bN} d \cdot k = m \quad \text{and} \quad K : \sum_{k : \bN} d \cdot k = n.\]
By $\Sigma$-induction, we may assume that $H$ is given by a pair $(h : \bN, p : d \cdot h = m)$ and $K$ is given by a pair $(k : \bN, q : d \cdot k = n)$. To get a term in $\sum_{x : \bN} d \cdot x = m + n$ we may use $x \coloneq h + k$. Our goal is then to define an identification $d \cdot (h + k) = m + n$ which we obtain as a concatenation
\[
\begin{tikzcd} d \cdot (h+ k ) \arrow[r, equals, "\term{dist}"] & d \cdot h + d \cdot k \arrow[r, equals, "\ap_{+d \cdot k}p"] & m + d \cdot k \arrow[r, equals, "\ap_{m+}q"] & m + n
\end{tikzcd}
\]
\end{proof}

We have observed many similarities between the rules of various type constructors and tautologies from logic. For instance, the elimination rule for the non-dependent function type supplies a function
\[ \term{modus-ponens} : A \times (A \to B) \to B.\]
One important difference is that general types may contain multiple terms that cannot be identified: i.e., for which it is possible to prove that $x =_A y \to \emptyset$. Later we'll study the following predicate on types:
\[ \term{is-prop}(A) \coloneq \prod_{x,y : A} x =_A y\]
which asserts that \emph{if} $A$ has multiple terms (which it may not) those terms can always be identified. This will be the $n=-1$ level of a hierarchy of $n$-types for $n \geq -2$.

\subsection*{The congruence relations on \texorpdfstring{$\bN$}{N}}

The family of identity types can be understood as a type-valued binary relation on a type.

\begin{defn} For a type $A$, a \textbf{typal binary relation} on $A$ is a family of types $x, y : A \vdash R(x,y) \univ$. A binary relation $R$ is 
\begin{itemize}
\item \textbf{reflexive} if it comes with a term $\rho : \prod_{x :A} R(x,x)$,
\item \textbf{symmetric} if it comes with a term $\sigma : \prod_{x,y :A} R(x,y) \to R(y,x)$,
\item \textbf{transitive} if it comes with a term $\tau : \prod_{x,y,z : A} \to R(x,y) \to R(y,z) \to R(x,z)$
\end{itemize}
A \textbf{typal equivalence relation} on $A$ is a reflexive, symmetric, and transitive, typal binary relation.
\end{defn}

For instance, for each $k : \bN$ we can define the relation of congruence modulo $k$ by defining a type
\[ x \equiv y \mod k\]
for each $x,y : \bN$ comprised of proofs that $x$ is equivalent to $y$ modulo $k$. Following Gauss, we say that $x$ is equivalent to $y$ mod $k$ if $k$ divides the symmetric difference $\term{dist}_\bN(x,y)$ defined recursively by
\[ \term{dist}_\bN(0,0) \coloneq 0 \quad \term{dist}_\bN(0, y+1) \coloneq y+1 \quad \term{dist}_\bN(x+1,0) \coloneq x+1, \quad \term{dist}_\bN(x+1,y+1) \coloneq \term{dist}_\bN(x,y).\]

\begin{defn} For $k,x,y : \bN$ define
\[ x \equiv y \mod k \coloneq k \mid \term{dist}_\bN(x,y).\]
Note this defines the \emph{type} $x \equiv y \mod k$. A term is then a pair comprised of an $\ell : \bN$ together with an identification $k \cdot \ell = \term{dist}_\bN(x,y)$.
\end{defn}

We leave the following to the course text:

\begin{prop} For each $k$, the typal relation $\equiv \mod k$ is an equivalence relation.
\end{prop}

\subsection*{The standard finite types}

The standard finite sets are classically defined as the sets $\{ n \in \bN \mid n < k\}$, so how do we interpret a subset $\{ x  \in A \mid P(x)\}$ characterized by a predicate in type theory?

In the Curry-Howard interpretation, the predicate $P(x)$ is interpreted as a type family and the type of terms $x$ in $A$ for which $P(x)$ is true is interpreted by the $\Sigma$-type $\sum_{x : A} P(x)$. Note for a general type family $P(x)$ it won't necessarily be the case that the map $\pr_1 \colon \sum_{x :A} P(x) \to A$ is a monomorphism\footnote{Though this will be the case if each type $P(x)$ is a proposition in the sense alluded to above.} so this construction operates a bit differently than in set theory.

Through this mechanism it is possible to define the classical finite sets as 
\[ \type{Classical-Fin}_k \coloneq \sum_{n : \bN}n < k \]
though the standard definition is as follows:

\begin{defn} We define the type family $\type{Fin}$ of \textbf{standard finite types} inductively (using the induction principle of $\bN$ and the universe $\UU$) as follows:
\[ \type{Fin}_0 \coloneq \emptyset, \quad \type{Fin}_{k+1} \coloneq \type{Fin}_k + \1.\]
\end{defn}

Write $i$ for $\inl : \type{Fin}_k \to \type{Fin}_{k+1}$ and $\star$ for the point $\inr(\star) : \type{Fin}_{k+1}$.

\subsection*{The natural numbers modulo $k+1$}

Given an equivalence relation $\sim$ on a set $A$ the quotient $A_{/\sim}$ comes equipped with a quotient map $q \colon A \to A_{/\sim}$ that satisfies two important properties:
\begin{enumerate}
\item $q$ is \textbf{effective}: $q(x) = q(y)$ if and only if $x \sim y$
\item $q$ is surjective: for all $[z] \in A_{/\sim}$ there is some $z \in A$ so that $q(z) = [z]$.
\end{enumerate}

Both properties can be expressed in type theory, though there are some subtleties.

\begin{defn} In the context of types $A$ and $B$ there is a type family $\type{is-surj} \colon (A \to B) \to \UU$ defined by
\[ \type{is-surj}(f) \coloneq \prod_{b : B} \sum_{a : A} f(a) =_B b.\]
\end{defn}

The subtlety is that this really defines a \emph{split} notion of surjectivity. A term $p$ in $\type{is-surj}(f)$ defines a function that for each term $b : B$ produces a term of $\sum_{a : A} f(a) =_B b$. By compositing $p$ with $\pr_1 : \sum_{a : A} f(a) =_B b \to A$, we obtain a function $s \colon B \to A$. By composing $p$ with $\pr_2 : \sum_{a : A} f(s(b)) =_B b$ we also obtain a proof that $s$ is a \textbf{section} of $f$. Thus surjective functions in homotopy type theory are really \textbf{split} surjective functions.

Challenge define $[-]_{k+1} \colon \bN \to \type{Fin}_{k+1}$.


\section*{September 29: Elementary number theory}

\part{The Univalent Foundations of Mathematics}

\section*{October 4: Equivalences}
\section*{October 6: Contractibility}
\section*{October 11: The fundamental theorem of identity types}
\section*{October 13: Propositions, sets, and general truncation levels}
\section*{October 18: Function extensionality}
\section*{October 20: Propositional truncation}
\section*{October 25: The image of a map}
\section*{October 27: Finite types}
\section*{November 1: The univalence axiom}
\section*{November 3: Set quotients}
\section*{November 8: Groups}
\section*{November 10: Algebra}
\section*{November 15: The real numbers}


\part{Synthetic Homotopy Theory}

\section*{November 17: The circle}
\section*{November 29: The universal cover of the circle}
\section*{December 1: Homotopy groups of types}
\section*{December 6: Classifying types of groups}





\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
